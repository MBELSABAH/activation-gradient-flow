# Activation Gradient Flow

This project examines the impact of various activation functions on gradient flow, saturation, and optimization stability in deep neural networks.

## Goals
- Compare classical and non-standard activation functions
- Analyze gradient magnitudes across layers
- Study saturation and dead-unit behavior
- Evaluate optimizerâ€“activation interactions

## Status
ðŸš§ Initial implementation in progress.

## Planned Activations
- Sigmoid
- Tanh
- ReLU
- Leaky ReLU
- Arctan
- Softsign
