# activation-gradient-flow
A controlled experimental study of activation functions in deep neural networks, focusing on gradient flow, saturation, dead units, and optimization dynamics rather than accuracy alone. Implemented from scratch in NumPy with detailed instrumentation.
